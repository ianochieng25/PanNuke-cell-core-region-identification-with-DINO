import requests
import argparse
import os
import sys
import glob
import subprocess
import numpy as np
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
from timm import create_model
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from dino.main_dino import train_dino
import random
import logging
from tqdm import tqdm
import cv2
import visualize_predictions  # Integrated visualization module
PROJECT_ROOT = os.path.abspath(os.path.dirname(__file__))

logging.basicConfig(
    filename=os.path.join(PROJECT_ROOT, 'pipeline.log'),
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
)

PANNUKE_ROOT = os.path.join(PROJECT_ROOT, 'data', 'pannuke')

ORIG_FOLD_IDX = {'train': 1, 'test': 2}

# è¨­å®šæ¨¡çµ„æœå°‹è·¯å¾‘ï¼Œç¢ºä¿è‡ªè¨‚å¥—ä»¶å¯è¢«æ‰¾åˆ°
PROJECT_ROOT = os.path.abspath(os.path.dirname(__file__))
sys.path.insert(0, os.path.join(PROJECT_ROOT, 'src'))
sys.path.insert(0, os.path.join(PROJECT_ROOT, 'preprocess'))


# æ¸…é™¤ checkpoints ä¸­çš„è¨“ç·´ç”¢ç‰©ï¼Œä½†ä¿ç•™é è¨“ç·´æ¬Šé‡

def reset_predictions():
    if os.path.exists(PRED_ROOT):
        shutil.rmtree(PRED_ROOT)
    os.makedirs(PRED_ROOT)
    print(f"ğŸ§¹ å·²æ¸…ç©ºé æ¸¬çµæœè³‡æ–™å¤¾ï¼š{PRED_ROOT}")

def clean_checkpoints_but_keep_pretrained():
    ckpt_path = os.path.join(PROJECT_ROOT, 'checkpoints')
    keep_filename = 'dino_vitsmall16_pretrain.pth'

    if not os.path.exists(ckpt_path):
        print(f"[â„¹ï¸] checkpoints è³‡æ–™å¤¾ä¸å­˜åœ¨ï¼Œç•¥éæ¸…ç†ã€‚")
        return

    # 1. æ”¶é›†è¦ä¿ç•™çš„æª”æ¡ˆå®Œæ•´è·¯å¾‘
    keep_files = []
    for root, dirs, files in os.walk(ckpt_path):
        for f in files:
            if f == keep_filename:
                keep_files.append(os.path.join(root, f))

    # 2. æ”¶é›†é€™äº›æª”æ¡ˆçš„æ‰€æœ‰ä¸Šå±¤è³‡æ–™å¤¾ï¼ˆåŒ…å«è‡ªå·±ï¼‰
    keep_dirs = set()
    for fpath in keep_files:
        dirpath = fpath
        while True:
            dirpath = os.path.dirname(dirpath)
            keep_dirs.add(dirpath)
            if dirpath == ckpt_path:
                break

    # 3. éæ­· checkpointsï¼Œåˆªæ‰ä¸åœ¨ä¿ç•™åå–®çš„æª”æ¡ˆèˆ‡è³‡æ–™å¤¾
    for root, dirs, files in os.walk(ckpt_path, topdown=False):
        for f in files:
            fpath = os.path.join(root, f)
            if fpath in keep_files:
                print(f"[âœ“] ä¿ç•™é è¨“ç·´æ¨¡å‹ï¼š{fpath}")
                continue
            try:
                os.remove(fpath)
                print(f"[ğŸ§¹] å·²åˆªé™¤æª”æ¡ˆï¼š{fpath}")
            except Exception as e:
                print(f"[âš ï¸] ç„¡æ³•åˆªé™¤ {fpath}ï¼š{e}")
        for d in dirs:
            dirpath = os.path.join(root, d)
            if dirpath in keep_dirs:
                print(f"[âœ“] ä¿ç•™è³‡æ–™å¤¾ï¼ˆå«é è¨“ç·´æ¨¡å‹ï¼‰ï¼š{dirpath}")
                continue
            try:
                shutil.rmtree(dirpath)
                print(f"[ğŸ§¹] å·²åˆªé™¤è³‡æ–™å¤¾ï¼š{dirpath}")
            except Exception as e:
                print(f"[âš ï¸] ç„¡æ³•åˆªé™¤ {dirpath}ï¼š{e}")



import shutil

def prompt_reset():
    choice = input("æ˜¯å¦é‡ç½®æ‰€æœ‰è¨“ç·´éç¨‹ï¼Ÿè¼¸å…¥ y é‡ç½®ï¼Œå…¶ä»–éµè·³éï¼š").strip().lower()
    if choice == 'y':
        print("æ­£åœ¨é‡ç½®è¨“ç·´è³‡æ–™...")
        # reset_split_dirs()  # Removed manual split reset
        # shuffle_and_split_pannuke() # Removed manual split logic
        clean_checkpoints_but_keep_pretrained()
        reset_predictions()
        clean_temp_files()
        print("âœ… é‡ç½®å®Œæˆï¼Œå°‡å¾é ­é–‹å§‹è¨“ç·´ã€‚\n")
    else:
        print("â­ï¸ è·³éé‡ç½®ï¼Œä¿ç•™ç¾æœ‰è¨“ç·´è³‡æ–™ã€‚\n")


from dataset.pannuke_dataset import PannukeDataset
from augment import get_training_augmentation, get_validation_augmentation

# é¡¯ç¤º PyTorch ç‰ˆæœ¬
print(f"PyTorch version: {torch.__version__}")

# è³‡æ–™å¤¾è¨­å®š
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
PANNUKE_ROOT = os.path.join(PROJECT_ROOT, "data", "pannuke")
PRED_ROOT = os.path.join(PROJECT_ROOT, "predictions")

ORIG_FOLD_IDX = {'train': 1, 'test': 2}

# å®˜æ–¹é è¨“ç·´æ¬Šé‡è·¯å¾‘èˆ‡ä¸‹è¼‰é€£çµ
PRETRAINED_PATH = os.path.join(PROJECT_ROOT, "checkpoints", "dino", "dino_deitsmall16_pretrain_full_checkpoint.pth")
PRETRAINED_URL = "https://dl.fbaipublicfiles.com/dino/dino_deitsmall16_pretrain/dino_deitsmall16_pretrain_full_checkpoint.pth"

# åŸå§‹ fold è·¯å¾‘ï¼šFold 1, 2, 3
RAW_FOLDS = [
    os.path.join(PANNUKE_ROOT, 'Fold 1'),
    os.path.join(PANNUKE_ROOT, 'Fold 2'),
    os.path.join(PANNUKE_ROOT, 'Fold 3'),
]

# shuffle å¾Œè¦å„²å­˜çš„æ–°ä½ç½®
FOLDS = {
    'train': os.path.join(PANNUKE_ROOT, 'train'),
    'test':  os.path.join(PANNUKE_ROOT, 'test'),
}




def setup_train_test_split():
    """
    Automatically organize Fold 1, 2, 3 into train and test directories.
    Train: Fold 1, Fold 2
    Test: Fold 3
    Renames files to {Fold}_images.npy etc. to avoid conflicts.
    """
    print("Checking dataset structure...")
    
    # Define source folds and their destination
    # You can adjust which fold goes where
    split_config = {
        'Fold 1': 'train',
        'Fold 2': 'train',
        'Fold 3': 'test'
    }
    
    for fold_name, split_name in split_config.items():
        src_fold = os.path.join(PANNUKE_ROOT, fold_name)
        dst_split = FOLDS[split_name]
        
        if not os.path.exists(src_fold):
            print(f"[Warn] Source {src_fold} does not exist. Skipping.")
            continue
            
        os.makedirs(dst_split, exist_ok=True)
        
        # Files to copy - Handle both "images.npy" and "1_images.npy"
        # We look for *images.npy, *masks.npy, *types.npy
        for suffix in ['images.npy', 'masks.npy', 'types.npy']:
            # Find files matching the suffix
            candidates = glob.glob(os.path.join(src_fold, f"*{suffix}"))
            
            for src_file in candidates:
                filename = os.path.basename(src_file)
                
                # Construct new name: Fold_1_images.npy or similar
                # If filename already has fold info (e.g. 1_images.npy), we can keep it or normalize it.
                # Let's normalize to Fold_X_images.npy to be safe and consistent.
                
                # Simple normalization:
                # If it starts with a digit (1_images.npy), prepend Fold_
                if filename[0].isdigit():
                     new_name = f"Fold_{filename}"
                else:
                     new_name = f"{fold_name.replace(' ', '_')}_{filename}"
                     
                dst_file = os.path.join(dst_split, new_name)
                
                if not os.path.exists(dst_file):
                    print(f"[Setup] Copying {src_file} -> {dst_file}")
                    try:
                        shutil.copy2(src_file, dst_file)
                    except Exception as e:
                        print(f"[Error] Failed to copy {src_file}: {e}")
                else:
                    # print(f"[Setup] {dst_file} already exists.")
                    pass

def merge_split_npy():
    """åˆä½µ train/val/test è³‡æ–™å¤¾è£¡çš„ *_images.npyã€*_types.npyã€*_masks.npy ç‚ºå–®ä¸€æª”æ¡ˆ"""
    for split, folder in FOLDS.items():
        # å®šç¾©è¦åˆä½µçš„ç›®æ¨™
        targets = [
            ('images.npy', '*_images.npy'),
            ('types.npy',  '*_types.npy'),
            ('masks.npy',  '*_masks.npy')
        ]
        
        for out_name, pattern in targets:
            files = sorted(glob.glob(os.path.join(folder, pattern)))
            # æ’é™¤å·²ç¶“åˆä½µå¥½çš„æª”æ¡ˆè‡ªå·± (é¿å…é‡è¤‡è®€å–)
            files = [f for f in files if os.path.basename(f) != out_name]
            
            if not files:
                continue
                
            out_path = os.path.join(folder, out_name)
            print(f"[merge] æ­£åœ¨åˆä½µ {out_name} åˆ° {out_path} ...")

            # 1. è¨ˆç®—ç¸½é•·åº¦èˆ‡å½¢ç‹€
            # 1. è¨ˆç®—ç¸½é•·åº¦èˆ‡å½¢ç‹€
            total_len = 0
            shapes = []
            dtype = None
            
            # å…ˆè®€å– metadataï¼ˆä¸ä½¿ç”¨ mmapï¼Œé¿å…å¥æŸ„è€—ç›¡ï¼‰
            for f in files:
                arr = np.load(f)  # ç›´æ¥è¼‰å…¥å®Œæ•´æª”æ¡ˆ
                if dtype is None:
                    dtype = arr.dtype
                total_len += arr.shape[0]
                shapes.append(arr.shape)
                del arr  # é‡‹æ”¾è¨˜æ†¶é«”
            
            if total_len == 0:
                continue
            
            # 2. å»ºç«‹è¼¸å‡ºçš„ memmap æª”æ¡ˆ
            # ä½¿ç”¨ç¬¬ä¸€å€‹æª”æ¡ˆçš„ shape[1:] ä½œç‚ºç‰¹å¾µç¶­åº¦
            final_shape = (total_len,) + shapes[0][1:]
            
            # å¦‚æœæª”æ¡ˆå·²å­˜åœ¨å…ˆåˆªé™¤
            if os.path.exists(out_path):
                os.remove(out_path)
            
            # å»ºç«‹ memmap
            merged = np.lib.format.open_memmap(out_path, mode='w+', dtype=dtype, shape=final_shape)
            
            # 3. åˆ†æ‰¹å¯«å…¥
            current_idx = 0
            for f in files:
                arr = np.load(f)
                n = arr.shape[0]
                merged[current_idx : current_idx + n] = arr[:]
                current_idx += n
                del arr  # é‡‹æ”¾è¨˜æ†¶é«”
            
            # ç¢ºä¿å¯«å…¥ç£ç¢Ÿ
            merged.flush()
            del merged
            print(f"  [OK] {out_name} åˆä½µå®Œæˆ (shape={final_shape})")





def clean_temp_files():
    temp_dirs = ['__pycache__', 'tmp', 'cache']
    for d in temp_dirs:
        dir_path = os.path.join(PROJECT_ROOT, d)
        if os.path.isdir(dir_path):
            shutil.rmtree(dir_path)
            print(f"ğŸ§¹ æ¸…é™¤æš«å­˜è³‡æ–™å¤¾ï¼š{dir_path}")

def find_types_npy(fold_dir):
    matches = glob.glob(os.path.join(fold_dir, "**", "*type*.npy"), recursive=True)
    return matches[0] if len(matches) == 1 else None

def count_labels(types_path):
    arr = np.load(types_path, allow_pickle=True).flatten()
    unique, counts = np.unique(arr, return_counts=True)
    return {str(u): int(c) for u, c in zip(unique, counts)}, arr.size

def count_distribution():
    print("=== PanNuke å„ Fold Patch åˆ†é¡æ¨™ç±¤åˆ†ä½ˆ ===\n")
    for split, folder in FOLDS.items():
        print(f"--- {split.upper():5s} ({folder}) ---")
        # å…ˆè©¦å–®ä¸€ types.npyï¼Œè‹¥ä¸å­˜åœ¨å†æ‰¾æ‰€æœ‰ *_types.npy
        types_paths = glob.glob(os.path.join(folder, "types.npy"))
        if not types_paths:
            types_paths = glob.glob(os.path.join(folder, "*_types.npy"))
        if not types_paths:
            print(f"  [Error] æ‰¾ä¸åˆ°ä»»ä½• types.npy æˆ– *_types.npy: {folder}", file=sys.stderr)
            continue

        # è¼‰å…¥ä¸¦åˆä½µ
        arrs = []
        for p in sorted(types_paths):
            try:
                arrs.append(np.load(p).flatten())
            except Exception as e:
                print(f"  [Warning] ç„¡æ³•è®€å– {p}ï¼š{e}", file=sys.stderr)
        if not arrs:
            print(f"  [Error] ç„¡å¯ç”¨çš„ label è³‡æ–™", file=sys.stderr)
            continue

        all_labels = np.concatenate(arrs)
        unique, counts = np.unique(all_labels, return_counts=True)
        total = all_labels.size

        print(f"  ç¸½æ¨£æœ¬æ•¸ï¼š{total}")
        print(f"  {'Label':>20s}{'Count':>10s}{'Percent':>10s}")
        print("  " + "-"*42)
        for lbl, cnt in sorted(zip(unique, counts), key=lambda x: x[1], reverse=True):
            print(f"  {str(lbl):>20s}{cnt:>10d}{cnt/total*100:>10.2f}%")
        print()

def find_pred_npy(split):
    cand = glob.glob(os.path.join(PRED_ROOT, f"{split}*_pred*.npy"))
    if len(cand) == 1:
        return cand[0]
    all_preds = glob.glob(os.path.join(PRED_ROOT, "*.npy"))
    if len(all_preds) == 1:
        return all_preds[0]
    pred_files = [p for p in all_preds if os.path.basename(p).startswith('pred')]
    return pred_files[0] if len(pred_files) == 1 else None

def evaluate_classification():
    for split, folder in FOLDS.items():
        print(f"=== {split.upper():5s} ({folder}) ===")
        types_path = find_types_npy(os.path.join(PANNUKE_ROOT, folder))
        if not types_path:
            print("  [skip] æ‰¾ä¸åˆ° types.npyï¼Œè·³éè©•ä¼°", file=sys.stderr)
            continue
        y_true = np.load(types_path).flatten()
        pred_path = find_pred_npy(split)
        if not pred_path or not os.path.isfile(pred_path):
            print(f"  [skip] æ‰¾ä¸åˆ°é æ¸¬æª”æ¡ˆ for {split}ï¼Œè·³éè©•ä¼°", file=sys.stderr)
            continue
        y_pred = np.load(pred_path).flatten()
        if y_true.shape != y_pred.shape:
            print(f"  [skip] true {y_true.shape} vs pred {y_pred.shape} ä¸ç¬¦ï¼Œè·³éè©•ä¼°", file=sys.stderr)
            continue
        acc = accuracy_score(y_true, y_pred)
        report = classification_report(y_true, y_pred, digits=4, zero_division=0)
        cm = confusion_matrix(y_true, y_pred)
        print(f"  Accuracy: {acc:.4f}\n")
        print("  Classification Report:"); [print(f"    {line}") for line in report.splitlines()]
        print("  Confusion Matrix:")
        print("   " + " ".join(f"{i:>4d}" for i in range(cm.shape[0])))
        for i, row in enumerate(cm): print(f"{i:>2d} " + " ".join(f"{c:>4d}" for c in row))
        print()

def step_done_dir(path: str) -> bool:
    return os.path.isdir(path) and len(os.listdir(path)) > 0

def step_done_file(path: str) -> bool:
    return os.path.isfile(path)

def run_script(rel_path, args=None, cwd=None):
    script = os.path.join(PROJECT_ROOT, rel_path)
    workdir = os.path.join(PROJECT_ROOT, cwd) if cwd else PROJECT_ROOT
    cmd = [sys.executable, script] + (args or [])
    logging.info(f">>> Running: {' '.join(cmd)} (cwd={workdir})")
    # æ”¹ç‚ºä¸ capture_outputï¼Œè®“å­ç¨‹åºçš„ stdout/stderr ç›´æ¥é¡¯ç¤ºåœ¨çµ‚ç«¯æ©Ÿ (åŒ…å« tqdm é€²åº¦æ¢)
    result = subprocess.run(cmd, cwd=workdir)
    if result.returncode != 0:
        logging.error(f"Command failed with return code {result.returncode}"); sys.exit(result.returncode)

def extract_vit_features(checkpoint_path, data_root, output_dir, batch_size=16):
    os.makedirs(output_dir, exist_ok=True)
    ds = PannukeDataset(data_root, aug=get_training_augmentation())  # â˜…åŠ å¼·åŒ–
    loader = DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)
    model = create_model('vit_small_patch8_224', pretrained=False, num_classes=0)
    ckpt = torch.load(checkpoint_path, map_location='cpu')
    state_dict = {k.replace('module.', ''): v for k,v in ckpt.get('student', ckpt).items()}
    model.load_state_dict(state_dict, strict=False)
    model.eval().cuda()
    all_cls, all_patches = [], []
    model.eval().cuda()
    all_cls, all_patches = [], []
    with torch.no_grad():
        for imgs, _, _ in tqdm(loader, desc='Extracting Features'):
            imgs = F.interpolate(imgs.cuda(non_blocking=True), size=(224,224), mode='bicubic', align_corners=False)
            tokens = model.forward_features(imgs)
            all_cls.append(tokens[:,0,:].cpu().numpy()); all_patches.append(tokens[:,1:,:].cpu().numpy())
    np.save(os.path.join(output_dir,'cls_tokens.npy'),np.concatenate(all_cls,axis=0)); np.save(os.path.join(output_dir,'patch_embeddings.npy'),np.concatenate(all_patches,axis=0))
    print(f"Extracted features: CLS {all_cls[0].shape}, patches {all_patches[0].shape} â†’ {output_dir}")

def run_pipeline():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu'); print(f"Using device: {device}")
    # 1. åŒ¯å‡º patches
    patches_dir = os.path.join(PROJECT_ROOT, 'preprocess', 'patches', 'train_patches')
    class0_dir = os.path.join(patches_dir, 'class0')
    if not step_done_dir(class0_dir):
        print(f"[Info] Patches missing or empty in {class0_dir}, running exporter...")
        # Clean up potential empty dir to be safe
        if os.path.exists(patches_dir):
            shutil.rmtree(patches_dir)
            
        run_script('preprocess/patch_exporter.py',
                   ['--fold_dir', os.path.join(PANNUKE_ROOT, 'train'),
                    '--output_dir', patches_dir])
    else:
        print(f"[skip] patches å·²å­˜åœ¨ï¼š{patches_dir}")

    # 2. DINO è‡ªç›£ç£é è¨“ç·´
    dino_ckpt = os.path.join(PROJECT_ROOT, 'checkpoints', 'dino', 'checkpoint.pth')
    if not step_done_file(dino_ckpt):
        dino_args = argparse.Namespace(
            data_path=patches_dir,
            output_dir=os.path.join(PROJECT_ROOT, 'checkpoints', 'dino'),
            arch="vit_small_patch16_224",
            patch_size=16,
            out_dim=1024,               # å®‰å…¨å…ˆç”¨ 1024ï¼ˆæˆ– 2048/4096 éƒ½è¡Œï¼‰
            batch_size_per_gpu=2,       # ä¸€å®šä¸è¦è¶…é 4ï¼Œå…ˆ 2 æœ€å®‰å…¨
            epochs=1,                   # åªè¨“ç·´ 1 epoch ç¢ºèª pipeline é€šé †
            warmup_teacher_temp=0.04,
            teacher_temp=0.07,
            warmup_teacher_temp_epochs=0,
            student_temp=0.1,
            local_crops_number=0,       # åªç”¢ç”Ÿ global cropï¼Œå®‰å…¨
            global_crops_scale=(0.4, 1.0),
            local_crops_scale=(0.05, 0.4),
            lr=0.0005,
            min_lr=1e-6,
            clip_grad=3.0,
            weight_decay=0.04,
            saveckp_freq=1,             # æ¯ epoch å­˜ checkpoint
            dist_url="tcp://127.0.0.1:29500",
            use_fp16=False,
            momentum_teacher=0.996,
            num_workers=2,
            freeze_last_layer=1               # åŠ é€™å€‹ï¼Œé¿å…å¤šç·šç¨‹æ¶è¨˜æ†¶é«”ï¼ˆæˆ–ç›´æ¥è¨­ 0ï¼‰
        )
        train_dino(dino_args)
    else:
        print(f"[skip] DINO checkpoint å·²å­˜åœ¨ï¼š{dino_ckpt}")

    # 3. ç‰¹å¾µæå–
    features_dir = os.path.join(PROJECT_ROOT, 'features', 'train_features')
    if not step_done_dir(features_dir):
        extract_vit_features(checkpoint_path=dino_ckpt,
                             data_root=os.path.join(PANNUKE_ROOT, 'train'),
                             output_dir=features_dir,
                             batch_size=16)
    else:
        print(f"[skip] features å·²å­˜åœ¨ï¼š{features_dir}")

    # 4. Segmentation è¨“ç·´
    seg_ckpt_dir = os.path.join(PROJECT_ROOT, 'checkpoints', 'segmentor')
    if not step_done_dir(seg_ckpt_dir):
        run_script('segmentor/train_segmentor.py',
                   ['--data_root', os.path.join(PANNUKE_ROOT),
                    '--checkpoint_dir', seg_ckpt_dir,
                    '--pretrained_ckpt', dino_ckpt,  # [Fix] Load DINO pretrained weights
                    '--batch_size', '4',
                    '--grad_accum_steps', '4',
                    '--num_workers', '0']) # [Fix] Set to 0 to avoid WinError 8
    else:
        print(f"[skip] segmentor checkpoints å·²å­˜åœ¨ï¼š{seg_ckpt_dir}")

    # 5. Segmentation æ¨è«–ï¼šå–®å¼µ NPY/PNG + å–®ä¸€å¤§é™£åˆ— NPY
    pred_single_dir = os.path.join(PROJECT_ROOT, 'predictions', 'single_masks')
    os.makedirs(pred_single_dir, exist_ok=True)
    pred_combined_npy = os.path.join(PRED_ROOT, 'pred_masks.npy')
    for split, folder in FOLDS.items():
        pred_npy = os.path.join(PRED_ROOT, f"{split}_pred_masks.npy")
        run_script(
            'segmentor/predict_segmentor.py',
            [
                '--weights', os.path.join(seg_ckpt_dir, 'model_final.pth'),
                '--input_dir', os.path.join(PANNUKE_ROOT, folder),
                '--output_dir', pred_single_dir,
                '--output_npy', pred_npy,
                '--batch_size', '8'
            ],
            cwd='segmentor'
        )

    # 6. ç‰¹å¾µå¯è¦–åŒ–
    viz_png = os.path.join(PROJECT_ROOT, 'results', 'tsne.png')
    if not step_done_file(viz_png):
        run_script('analysis/feature_visualizer.py',
                ['--features_dir', features_dir,
                    '--output_png', viz_png])
        clean_temp_files()
    else:
        print(f"[skip] å¯è¦–åŒ–çµæœå·²å­˜åœ¨ï¼š{viz_png}")
        clean_temp_files()

    # 7. é æ¸¬çµæœå¯è¦–åŒ– (Overlay Masks)
    print("\n=== ç”Ÿæˆé æ¸¬çµæœè¦–è¦ºåŒ–åœ– (Overlay) ===")
    for split in ['train', 'test']:
        visualize_predictions.visualize_split(split, num_samples=10)




def clean_mask(mask_binary):
    """
    Apply morphological opening to remove small noise.
    mask_binary: (N, H, W) or (H, W) numpy array, uint8
    """
    kernel = np.ones((3,3), np.uint8)
    # Ensure uint8
    if mask_binary.dtype != np.uint8:
        mask_binary = mask_binary.astype(np.uint8)
        
    if mask_binary.ndim == 2:
        return cv2.morphologyEx(mask_binary, cv2.MORPH_OPEN, kernel)
    
    cleaned = np.zeros_like(mask_binary)
    for i in range(mask_binary.shape[0]):
        cleaned[i] = cv2.morphologyEx(mask_binary[i], cv2.MORPH_OPEN, kernel)
    return cleaned

def evaluate_segmentation_for_fold(fold_name, fold_folder, num_classes=6):
    # ç¢ºä¿å·²åˆä½µ shards ç‚ºå–®ä¸€æª”æ¡ˆ
    merge_split_npy()

    print(f"=== {fold_name} ({fold_folder}) åˆ†å‰²çµæœè©•ä¼° ===")
    # 1. ç›´æ¥å¾ split è³‡æ–™å¤¾è®€ GT masks.npy
    gt_path = os.path.join(fold_folder, "masks.npy")
    if not os.path.isfile(gt_path):
        print(f"[Warn] GT masks.npy ä¸å­˜åœ¨ï¼ŒåŸ·è¡Œ merge_split_npy()", file=sys.stderr)
        merge_split_npy()
        if not os.path.isfile(gt_path):
            print(f"[Error] ä»æ‰¾ä¸åˆ° GT masks.npy: {gt_path}", file=sys.stderr)
            return

    print(f"=== {fold_name} ({fold_folder}) åˆ†å‰²çµæœè©•ä¼° ===")
    # 1. ç›´æ¥å¾ split è³‡æ–™å¤¾è®€ GT masks.npy
    gt_path = os.path.join(fold_folder, "masks.npy")
    if not os.path.isfile(gt_path):
        print(f"[Warn] GT masks.npy ä¸å­˜åœ¨ï¼ŒåŸ·è¡Œ merge_split_npy()", file=sys.stderr)
        merge_split_npy()
        if not os.path.isfile(gt_path):
            print(f"[Error] ä»æ‰¾ä¸åˆ° GT masks.npy: {gt_path}", file=sys.stderr)
            return

    # 2. è®€é æ¸¬çµæœ
    pred_path = os.path.join(PRED_ROOT, f"{fold_name}_pred_masks.npy")
    if not os.path.isfile(pred_path):
        print(f"[Error] æ‰¾ä¸åˆ°é æ¸¬æª”: {pred_path}", file=sys.stderr)
        return

    # ä½¿ç”¨ mmap è®€å–
    gt_mmap = np.load(gt_path, mmap_mode='r')
    pred_mmap = np.load(pred_path, mmap_mode='r')

    if gt_mmap.shape[0] != pred_mmap.shape[0]:
        print(f"[Error] GT count {gt_mmap.shape[0]} vs Pred count {pred_mmap.shape[0]} ä¸ç¬¦", file=sys.stderr)
        return

    # åˆ†å¡Šè¨ˆç®— IoU
    # Binary: Class 0 (Background), Class 1 (Cells)
    # Intersection & Union accumulators
    inter_acc = {0: 0, 1: 0}
    union_acc = {0: 0, 1: 0}
    
    chunk_size = 1000
    total = gt_mmap.shape[0]
    
    print(f"Evaluating Binary Segmentation (0=Bg, 1=Cells) in chunks...")
    
    for start_idx in range(0, total, chunk_size):
        end_idx = min(start_idx + chunk_size, total)
        
        # Load chunk
        gt_chunk = gt_mmap[start_idx:end_idx]
        pred_chunk = pred_mmap[start_idx:end_idx]
        
        # Handle GT format
        if gt_chunk.ndim == 4:
            gt_chunk = np.argmax(gt_chunk, axis=-1)
            
        # Handle Pred format
        if pred_chunk.ndim == 4:
            pred_chunk = np.argmax(pred_chunk, axis=-1)
            
        # Convert GT to Binary
        # Original 3 (Dead), 5 (Background) -> 0 (Background)
        # Original 0, 1, 2, 4 -> 1 (Cells)
        gt_binary = np.zeros_like(gt_chunk, dtype=np.uint8)
        is_cell_gt = (gt_chunk != 3) & (gt_chunk != 5)
        gt_binary[is_cell_gt] = 1
        
        # Convert Pred to Binary (if needed)
        pred_binary = pred_chunk
        if pred_chunk.max() > 1:
             pred_binary = np.zeros_like(pred_chunk, dtype=np.uint8)
             is_cell_pred = (pred_chunk != 3) & (pred_chunk != 5)
             pred_binary[is_cell_pred] = 1
        
        # Apply noise filtering
        pred_binary = clean_mask(pred_binary)
        
        # Accumulate
        for cls in [0, 1]:
            inter = np.logical_and(gt_binary == cls, pred_binary == cls).sum()
            union = np.logical_or(gt_binary == cls, pred_binary == cls).sum()
            inter_acc[cls] += inter
            union_acc[cls] += union

    # Calculate Final IoU and Dice
    class_ious = []
    class_dices = []
    
    # Also accumulate total pred and gt pixels for Dice
    pred_acc = {0: 0, 1: 0}
    gt_acc = {0: 0, 1: 0}
    
    # Recalculate to get pred and gt counts
    for start_idx in range(0, total, chunk_size):
        end_idx = min(start_idx + chunk_size, total)
        gt_chunk = gt_mmap[start_idx:end_idx]
        pred_chunk = pred_mmap[start_idx:end_idx]
        
        if gt_chunk.ndim == 4:
            gt_chunk = np.argmax(gt_chunk, axis=-1)
        if pred_chunk.ndim == 4:
            pred_chunk = np.argmax(pred_chunk, axis=-1)
            
        gt_binary = np.zeros_like(gt_chunk, dtype=np.uint8)
        is_cell_gt = (gt_chunk != 3) & (gt_chunk != 5)
        gt_binary[is_cell_gt] = 1
        
        pred_binary = pred_chunk
        if pred_chunk.max() > 1:
            pred_binary = np.zeros_like(pred_chunk, dtype=np.uint8)
            is_cell_pred = (pred_chunk != 3) & (pred_chunk != 5)
            pred_binary[is_cell_pred] = 1
            
        # Apply noise filtering
        pred_binary = clean_mask(pred_binary)
            
        for cls in [0, 1]:
            pred_acc[cls] += (pred_binary == cls).sum()
            gt_acc[cls] += (gt_binary == cls).sum()
    
    print(f"\n{'='*60}")
    print(f"  {fold_name.upper()} Segmentation Metrics")
    print(f"{'='*60}")
    print(f"{'Class':<15} {'IoU':>10} {'Dice':>10}")
    print(f"{'-'*60}")
    
    for cls in [0, 1]:
        i = inter_acc[cls]
        u = union_acc[cls]
        p = pred_acc[cls]
        g = gt_acc[cls]
        
        iou = i / u if u > 0 else float('nan')
        dice = (2 * i) / (p + g) if (p + g) > 0 else float('nan')
        
        class_ious.append(iou)
        class_dices.append(dice)
        
        cls_name = "Background" if cls == 0 else "Cells"
        print(f"{cls_name:<15} {iou:>10.4f} {dice:>10.4f}")

    # å¹³å‡ IoU å’Œ Dice
    miou = np.nanmean(class_ious)
    mdice = np.nanmean(class_dices)
    print(f"{'-'*60}")
    print(f"{'Mean':<15} {miou:>10.4f} {mdice:>10.4f}")
    print(f"{'='*60}\n")


def evaluate_all_splits():
    for split, folder in FOLDS.items():
        evaluate_segmentation_for_fold(split, folder, num_classes=2)

    # Download code removed as it is unnecessary here and causing 403 errors
    pass




def need_seg_train() -> bool:
    """è‹¥ checkpoints/segmentor/model_final.pth ä¸å­˜åœ¨å‰‡å›å‚³ True"""
    seg_ckpt = os.path.join(PROJECT_ROOT, 'checkpoints', 'segmentor', 'model_final.pth')
    return not os.path.isfile(seg_ckpt)

def finetune_with_pretrained(pretrained_ckpt):
    # 0. è©¢å•æ˜¯å¦è¦é€²è¡Œ segmentation å¾®èª¿è¨“ç·´
    while True:
        ft_choice = input("[å¾®èª¿] æ˜¯å¦è¦é€²è¡Œ segmentation å¾®èª¿è¨“ç·´ï¼Ÿ(y/n): ").strip().lower()
        if ft_choice in ('y', 'n'):
            break
        print("è«‹è¼¸å…¥ y æˆ– nã€‚")

    seg_ckpt_dir = os.path.join(PROJECT_ROOT, 'checkpoints', 'segmentor')
    os.makedirs(seg_ckpt_dir, exist_ok=True)

    if ft_choice == 'y':
        print("ğŸ—‘ï¸ åˆªé™¤èˆŠæ¨¡å‹ä¸¦é‡æ–°å¾®èª¿è¨“ç·´...")
        shutil.rmtree(seg_ckpt_dir, ignore_errors=True)
        os.makedirs(seg_ckpt_dir, exist_ok=True)
        run_script(
            'segmentor/train_segmentor.py',
            [
                '--data_root', os.path.join(PANNUKE_ROOT),
                '--checkpoint_dir', seg_ckpt_dir,
                '--pretrained_ckpt', pretrained_ckpt,
                '--batch_size', '4',
                '--grad_accum_steps', '4'
            ]
        )
    else:
        if need_seg_train():
            print("[æ³¨æ„] æœªæ‰¾åˆ° segmentation checkpointï¼Œå°‡è‡ªå‹•å•Ÿå‹•è¨“ç·´ã€‚")
            train_segmentor_only()
        else:
            print("[å¾®èª¿] è·³é segmentation å¾®èª¿è¨“ç·´ã€‚")

    # 1. ç„¡è«–æ˜¯å¦è¨“ç·´ï¼Œéƒ½è¦æ¨äºŒå€‹ splits
    print("[å¾®èª¿] é–‹å§‹æ¨è«–äºŒå€‹ splits ...")
    pred_single_dir = os.path.join(PRED_ROOT, 'single_masks')
    os.makedirs(pred_single_dir, exist_ok=True)
    for split, folder in FOLDS.items():
        pred_npy = os.path.join(PRED_ROOT, f"{split}_pred_masks.npy")
        run_script(
            'segmentor/predict_segmentor.py',
            [
                '--weights', os.path.join(seg_ckpt_dir, 'model_final.pth'),
                '--input_dir', os.path.join(PANNUKE_ROOT, folder),
                '--output_dir', pred_single_dir,
                '--output_npy', pred_npy,
                '--batch_size', '8'
            ],
            cwd='segmentor'
        )

    # 2. åˆ†å‰²è©•ä¼°
    print("[å¾®èª¿] è©•ä¼°åˆ†å‰²çµæœ ...")
    evaluate_all_splits()

    # 3. è¦–è¦ºåŒ–
    print("[å¾®èª¿] ç”Ÿæˆè¦–è¦ºåŒ–çµæœ ...")
    for split in ['train', 'test']:
        visualize_predictions.visualize_split(split, num_samples=10)


def select_mode():
    """äº’å‹•å¼é¸æ“‡æ¨¡å¼ï¼Œè¿”å› 'train', 'finetune' æˆ– 'inference'"""
    while True:
        print("\nè«‹é¸æ“‡æ¨¡å¼ï¼š")
        print("  1. è‡ªå·±è¨“ç·´ DINO (train)")
        print("  2. ä¸‹è¼‰å®˜æ–¹ DINO é è¨“ç·´æ¬Šé‡å¾®èª¿åˆ†å‰² (finetune)")
        print("  3. ä½¿ç”¨ç¾æœ‰ checkpoint é€²è¡Œæ¨è«– (inference)")
        choice = input("è«‹è¼¸å…¥ 1, 2 æˆ– 3ï¼š").strip()
        if choice == '1':
            return 'train'
        if choice == '2':
            return 'finetune'
        if choice == '3':
            return 'inference'
        print("è¼¸å…¥éŒ¯èª¤ï¼Œè«‹é‡æ–°è¼¸å…¥ã€‚")

def ensure_official_pretrained():
    """ç¢ºä¿å®˜æ–¹é è¨“ç·´æ¬Šé‡å·²ä¸‹è¼‰ï¼Œå¦å‰‡æç¤ºä¸‹è¼‰"""
    if not os.path.isfile(PRETRAINED_PATH):
        print(f"æœªæª¢æ¸¬åˆ°å®˜æ–¹é è¨“ç·´æ¬Šé‡ï¼Œè«‹å…ˆä¸‹è¼‰\n{PRETRAINED_URL}\nä¸¦æ”¾åˆ°:\n {PRETRAINED_PATH}")
        input("ä¸‹è¼‰å®Œæˆå¾Œè«‹æŒ‰ Enter ç¹¼çºŒ...")
    else:
        print(f"å·²ç™¼ç¾é è¨“ç·´æ¬Šé‡ï¼š{PRETRAINED_PATH}")

def main_loop():

    while True:
        mode = select_mode()
        # é‡ç½®é¸é …
        reset_choice = input("æ˜¯å¦é‡ç½®æ‰€æœ‰è¨“ç·´éç¨‹ï¼Ÿè¼¸å…¥ y é‡ç½®ï¼Œå…¶ä»–éµè·³éï¼š").strip().lower()
        if reset_choice == 'y':
            prompt_reset()
        else:
            # æœªé‡ç½®ï¼Œç¢ºä¿å·²åˆ‡åˆ†è³‡æ–™
            if not step_done_file(os.path.join(FOLDS['train'], 'images.npy')):
                print("â„¹ï¸ æ­£åœ¨è‡ªå‹•è¨­å®š Train/Test è³‡æ–™å¤¾...")
                setup_train_test_split()
                merge_split_npy()
        # é¡¯ç¤ºåˆ†å¸ƒ
        count_distribution()

        if mode == 'train':
            print("ã€æ¨¡å¼ï¼šè‡ªå·±è¨“ç·´ DINOã€‘")
            run_pipeline()
            evaluate_all_splits()

        elif mode == 'finetune':
            print("ã€æ¨¡å¼ï¼šå®˜æ–¹é è¨“ç·´å¾®èª¿ã€‘")
            ensure_official_pretrained()
            finetune_with_pretrained(PRETRAINED_PATH)
            evaluate_all_splits()
        
        elif mode == 'inference':
            print("ã€æ¨¡å¼ï¼šä½¿ç”¨ç¾æœ‰ checkpoint æ¨è«–ã€‘")
            default_ckpt = os.path.join(PROJECT_ROOT, 'checkpoints', 'segmentor', 'model_final.pth')
            ckpt_path = input(f"è«‹è¼¸å…¥ checkpoint è·¯å¾‘ (é è¨­: {default_ckpt}): ").strip()
            if not ckpt_path:
                ckpt_path = default_ckpt
            
            if not os.path.isfile(ckpt_path):
                print(f"âŒ æ‰¾ä¸åˆ°æª”æ¡ˆï¼š{ckpt_path}")
                continue
                
            print(f"ä½¿ç”¨ checkpoint: {ckpt_path}")
            
            # æ¨è«–
            pred_single_dir = os.path.join(PRED_ROOT, 'single_masks')
            os.makedirs(pred_single_dir, exist_ok=True)
            for split, folder in FOLDS.items():
                pred_npy = os.path.join(PRED_ROOT, f"{split}_pred_masks.npy")
                run_script(
                    'segmentor/predict_segmentor.py',
                    [
                        '--weights', ckpt_path,
                        '--input_dir', os.path.join(PANNUKE_ROOT, folder),
                        '--output_dir', pred_single_dir,
                        '--output_npy', pred_npy,
                        '--batch_size', '8'
                    ],
                    cwd='segmentor'
                )
            
            # è©•ä¼°
            evaluate_all_splits()

            # è¦–è¦ºåŒ–
            print("ç”Ÿæˆè¦–è¦ºåŒ–çµæœ ...")
            for split in ['train', 'test']:
                visualize_predictions.visualize_split(split, num_samples=10)

        # æ˜¯å¦å†æ¬¡åŸ·è¡Œ
        again = input("æ˜¯å¦è¦å†æ¬¡åŸ·è¡Œæµç¨‹ï¼Ÿ(y/n): ").strip().lower()
        if again != 'y':
            print("æµç¨‹çµæŸã€‚")
            break





def ensure_dino_checkpoint():
    """ç¢ºä¿å·²å­˜åœ¨è¨“ç·´å¥½çš„ DINO æ¬Šé‡ï¼›è‹¥ç„¡å‰‡åœæ­¢åŸ·è¡Œã€‚"""
    dino_dir = os.path.join(PROJECT_ROOT, 'checkpoints', 'dino')
    ckpt = os.path.join(dino_dir, 'dino_vitsmall16_pretrain.pth')
    if os.path.isfile(ckpt):
        print(f'[OK] DINO checkpoint found: {ckpt}')
        return ckpt
    raise FileNotFoundError('ç¼ºå°‘ DINO æ¬Šé‡ï¼Œè«‹å…ˆå®Œæˆ DINO é è¨“ç·´æˆ–ä¸‹è¼‰å®˜æ–¹æ¬Šé‡ã€‚')

def train_segmentor_only():
    """åƒ…åŸ·è¡Œ segmentation è¨“ç·´èˆ‡é©—è­‰"""
    seg_ckpt_dir = os.path.join(PROJECT_ROOT, 'checkpoints', 'segmentor')
    os.makedirs(seg_ckpt_dir, exist_ok=True)

    # 1) è¨“ç·´ segmentation
    dino_ckpt = ensure_dino_checkpoint()
    run_script(
        'segmentor/train_segmentor.py',
        [
            '--data_root', PANNUKE_ROOT,
            '--checkpoint_dir', seg_ckpt_dir,
            '--pretrained_ckpt', dino_ckpt,
            '--batch_size', '4',
            '--grad_accum_steps', '4'
        ],
        cwd='segmentor'
    )

    # 2) æ¨è«– segmentation
    pred_single_dir = os.path.join(PRED_ROOT, 'single_masks')
    os.makedirs(pred_single_dir, exist_ok=True)
    for split, folder in FOLDS.items():
        pred_npy = os.path.join(PRED_ROOT, f"{split}_pred_masks.npy")
        run_script(
            'segmentor/predict_segmentor.py',
            [
                '--weights', os.path.join(seg_ckpt_dir, 'model_final.pth'),
                '--input_dir', os.path.join(PANNUKE_ROOT, folder),
                '--output_dir', pred_single_dir,
                '--output_npy', pred_npy,
                '--batch_size', '8'
            ],
            cwd='segmentor'
        )

    # 3) è©•ä¼°
    evaluate_all_splits()

if __name__ == "__main__":
    main_loop()
